import os
import json
import requests
import asyncio
import re
from telegram import Bot, Update
from telegram.ext import ApplicationBuilder, MessageHandler, ContextTypes, filters
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from newspaper import Article

# Load environment variables
load_dotenv()
BOT_TOKEN = os.environ["BOT_TOKEN"]
GOOGLE_API_KEY = os.environ["GOOGLE_API_KEY"]
GOOGLE_CSE_ID = os.environ["GOOGLE_CSE_ID"]

# File paths
SUBSCRIBERS_FILE = "subscribers.json"
SENT_FILE = "sent_articles.json"
COMPANY_FILE = "companies.json"
TOPIC_FILE = "topics.json"

# --- Initialize companies.json with default list if not present ---
def init_default_companies():
    if not os.path.exists(COMPANY_FILE):
        default_companies = [
            "í•œêµ­ì½œë§ˆ", "í•œì§„ì¹¼", "í˜„ëŒ€í•´ìƒ", "ë™ì–‘ê³ ì†", "ì‚¼ì˜ì „ìž", "ë‚¨ì–‘ìœ ì—…", "ë§µìŠ¤ë¦¬ì–¼í‹°", "ìƒìƒì¸",
            "íŒŒí¬ì‹œìŠ¤í…œìŠ¤", "ê¸ˆí˜¸ì„ìœ í™”í•™", "ì™€ì´ì— ", "ì”¨ì—ìŠ¤ìœˆë“œ", "ì‚¬ì¡°ì˜¤ì–‘", "ë§ˆìŒAI", "ë¦¬íŒŒì¸", "ê°€ë¹„ì•„"
        ]
        with open(COMPANY_FILE, "w") as f:
            json.dump(default_companies, f, ensure_ascii=False, indent=2)

# --- Helpers: Load/Save JSON files ---
def load_json(filename: str) -> list:
    if not os.path.exists(filename):
        return []
    with open(filename, "r") as f:
        return json.load(f)

def save_json(filename: str, data: list):
    with open(filename, "w") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# Loaders for specific files
def load_subscribers(): return load_json(SUBSCRIBERS_FILE)
def save_subscribers(data): save_json(SUBSCRIBERS_FILE, data)
def load_sent_articles(): return load_json(SENT_FILE)
def save_sent_articles(data): save_json(SENT_FILE, data)
def load_companies(): return load_json(COMPANY_FILE)
def save_companies(data): save_json(COMPANY_FILE, data)
def load_topics(): return load_json(TOPIC_FILE)

# --- Google Search ---
def google_search_all(query: str, api_key: str, cse_id: str, days: int = 7) -> list:
    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": api_key,
        "cx": cse_id,
        "q": query,
        "num": 10,
        "lr": "lang_ko",
        "gl": "kr",
        
        "dateRestrict": f"d{days}"
    }
    try:
        res = requests.get(url, params=params)
        return res.json().get("items", [])
    except Exception as e:
        print(f"[!] google_search_all error: {e}")
        return []

# --- Content Extraction ---
def extract_clean_text(url: str) -> str:
    try:
        article = Article(url, language='ko')
        article.download()
        article.parse()
        return article.text.strip()
    except Exception:
        try:
            res = requests.get(url, timeout=5)
            soup = BeautifulSoup(res.text, "html.parser")
            return soup.get_text(separator=' ', strip=True)[:3000]
        except Exception as e:
            print(f"[!] Fallback extraction failed: {e}")
            return ""

# --- Similarity Detection ---
def is_similar_article(new_text: str, old_texts: list, threshold=0.6):
    new_tokens = set(re.findall(r"\w+", new_text))
    for old in old_texts:
        old_tokens = set(re.findall(r"\w+", old))
        if not old_tokens:
            continue
        similarity = len(new_tokens & old_tokens) / len(new_tokens | old_tokens)
        if similarity >= threshold:
            return True
    return False

# --- Bot Behavior: News Push ---
async def push_news():
    bot = Bot(token=BOT_TOKEN)
    subscribers = load_subscribers()
    sent_articles = load_sent_articles()
    sent_summaries = [a["summary"] for a in sent_articles]
    companies = load_companies()
    topics = load_topics()

    for chat_id in subscribers:
        for company in companies:
            try:
                results = google_search_all(company, GOOGLE_API_KEY, GOOGLE_CSE_ID)
                for item in results:
                    link = item.get("link", "")
                    if any(link == a["url"] for a in sent_articles):
                        continue

                    content = extract_clean_text(link)
                    if not content:
                        continue

                    summary = content[:1000]
                    if is_similar_article(summary, sent_summaries):
                        continue

                    topic_hits = [t for t in topics if t in content]
                    if (company in content and topic_hits) or len(topic_hits) >= 2:
                        message = f"ðŸ” {company} + {len(topic_hits)}ê°œ í† í”½\n{item.get('title')}\n{link}"
                        await bot.send_message(chat_id=chat_id, text=message)

                        sent_articles.append({"url": link, "summary": summary})
                        sent_summaries.append(summary)
                        save_sent_articles(sent_articles)
            except Exception as e:
                print(f"[!] Error for {company}: {e}")

# --- Bot Behavior: Add Keyword + Auto-subscribe ---
def is_command(text: str) -> bool:
    return text.startswith("/")

async def handle_text(update: Update, context: ContextTypes.DEFAULT_TYPE):
    chat_id = update.effective_chat.id
    msg = update.message.text.strip()

    # Auto-subscribe
    subscribers = load_subscribers()
    if chat_id not in subscribers:
        subscribers.append(chat_id)
        save_subscribers(subscribers)
        await context.bot.send_message(chat_id=chat_id, text="ðŸ‘‹ Welcome! You've been subscribed to news updates.")

    # Skip commands
    if is_command(msg):
        return

    # Add to companies.json
    companies = load_companies()
    if msg in companies:
        await context.bot.send_message(chat_id=chat_id, text=f"âœ… ì´ë¯¸ ë“±ë¡ëœ í‚¤ì›Œë“œìž…ë‹ˆë‹¤: {msg}")
    else:
        companies.append(msg)
        save_companies(companies)
        await context.bot.send_message(chat_id=chat_id, text=f"ðŸŽ‰ í‚¤ì›Œë“œê°€ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤: {msg}")

# --- Main Entry Point ---
if __name__ == "__main__":
    import sys
    init_default_companies()  # ì´ˆê¸° companies.json ì„¸íŒ…
    if len(sys.argv) > 1 and sys.argv[1] == "run-bot":
        app = ApplicationBuilder().token(BOT_TOKEN).build()
        app.add_handler(MessageHandler(filters.TEXT & (~filters.COMMAND), handle_text))
        print("ðŸ¤– Bot is polling...")
        app.run_polling()
    else:
        asyncio.run(push_news())


'''import os
import json
import requests
import asyncio
from telegram import Bot, Update
from telegram.ext import ApplicationBuilder, MessageHandler, ContextTypes, filters
from dotenv import load_dotenv
import re
from bs4 import BeautifulSoup
from newspaper import Article

# load secrets

load_dotenv()
BOT_TOKEN = os.environ["BOT_TOKEN"]
GOOGLE_API_KEY = os.environ["GOOGLE_API_KEY"]
GOOGLE_CSE_ID = os.environ["GOOGLE_CSE_ID"]
SUBSCRIBERS_FILE = "subscribers.json"
SENT_FILE = "sent_articles.json"

# search keywords 
companies = ['í•œêµ­ì½œë§ˆ', 'í•œì§„ì¹¼', 'í˜„ëŒ€í•´ìƒ', 'ë™ì–‘ê³ ì†', "ì‚¼ì˜ì „ìž", "ë‚¨ì–‘ìœ ì—…", "ë§µìŠ¤ë¦¬ì–¼í‹°", "ìƒìƒì¸",
             "íŒŒí¬ì‹œìŠ¤í…œìŠ¤", "ê¸ˆí˜¸ì„ìœ í™”í•™", "ì™€ì´ì— ", "ì”¨ì—ìŠ¤ìœˆë“œ", "ì‚¬ì¡°ì˜¤ì–‘", "ë§ˆìŒAI", "ë¦¬íŒŒì¸", "ê°€ë¹„ì•„"]

topics = ["í–‰ë™ì£¼ì˜", "ì†Œì•¡ì£¼ì£¼", "ê²½ì˜ê¶Œ ë¶„ìŸ", 'ë°¸ë¥˜ì—…', 'ì§€ë°°êµ¬ì¡°', 'ì£¼ì£¼ì´íšŒ','ì•¡í‹°ë¹„ìŠ¤íŠ¸', 'ìµœëŒ€ì£¼ì£¼', 
          'ê¸°ì—…ê°€ì¹˜ ì œê³ ','ì£¼ì£¼ê°€ì¹˜ ì œê³ ', "ê¸°ì—…ë¶„í• ", "ë¬¼ì ë¶„í• ", "ì¸ì ë¶„í• ","ìžì‚¬ì£¼", "ë°°ë‹¹", "ë°°ë‹¹í™•ëŒ€",
          "ë°°ë‹¹ì •ì±…", "ì§€ë¶„í™•ëŒ€", "ì§€ë¶„ë§¤ìž…", "ê²½ì˜ì°¸ì—¬", "ì‚¬ì™¸ì´ì‚¬","ì´ì‚¬íšŒ", "ê²½ì˜íˆ¬ëª…ì„±","ê¸°ì—…ì§€ë°°êµ¬ì¡°", 
          "ì˜ê²°ê¶Œ", "ê²½ì˜íš¨ìœ¨í™”", "ì‚¬ì—…ìž¬íŽ¸", "ì§€ì†ê°€ëŠ¥ê²½ì˜","ì´ìµí™˜ì›", "ë¦¬ìŠ¤í¬ê´€ë¦¬", "ê²½ì˜ì‡„ì‹ ", 
          "ê°ì‚¬ìœ„ì› ë¶„ë¦¬ì„ ì¶œ", "ì§‘ì¤‘íˆ¬í‘œì œ", "ëˆ„ì íˆ¬í‘œì œ", "ì „ìžíˆ¬í‘œ", "ì´ì‚¬í›„ë³´ì¶”ì²œìœ„ì›íšŒ","ì£¼ì£¼ì œì•ˆ", 
          "ì§€ë¶„ìœ¨ ë³€í™”", "ìµœëŒ€ì£¼ì£¼ ë³€ê²½", "ì˜¤ë„ˆë¦¬ìŠ¤í¬", "ì˜¤ë„ˆì¼ê°€", "íŠ¹ìˆ˜ê´€ê³„ì¸", "ìƒì†ì„¸", "ìš°í˜¸ì§€ë¶„", 
          "ì˜ê²°ê¶Œ ëŒ€ë¦¬í–‰ì‚¬", "ì´ìµìž‰ì—¬ê¸ˆ", "ë°°ë‹¹ì„±í–¥", "í˜„ê¸ˆë°°ë‹¹", "í˜„ê¸ˆíë¦„ í™œìš©", "ì£¼ì£¼í™˜ì›", "ë°±ê¸°ì‚¬", 
          "ì ëŒ€ì  ì¸ìˆ˜í•©ë³‘", "ê²½ì˜ê¶Œ ë°©ì–´", "ì°¨ë“±ì˜ê²°ê¶Œ", "ê³µê°œë§¤ìˆ˜", "ì§€ë¶„ë§¤ìˆ˜ì²­êµ¬"]


def load_subscribers() -> list[int]:
    if not os.path.exists(SUBSCRIBERS_FILE):
        return []
    with open(SUBSCRIBERS_FILE, "r") as f:
        return json.load(f)

def save_subscribers(chat_ids: list[int]) -> None:
    with open(SUBSCRIBERS_FILE, "w") as f:
        json.dump(chat_ids, f)

async def auto_subscribe(update: Update, context: ContextTypes.DEFAULT_TYPE):
    chat_id = update.effective_chat.id
    subscribers = load_subscribers()
    if chat_id not in subscribers:
        subscribers.append(chat_id)
        save_subscribers(subscribers)
        await context.bot.send_message(chat_id=chat_id, text="ðŸ‘‹ Welcome! You've been subscribed to news updates.")

def google_search_all(query: str, api_key: str, cse_id: str, days: int = 2) -> list[dict]:
    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": api_key,
        "cx": cse_id,
        "q": query,
        "num": 10,
        "lr": "lang_ko",
        "gl": "kr"
    }
    if days:
        params["dateRestrict"] = f"d{days}"
    try:
        response = requests.get(url, params=params)
        data = response.json()
        return data.get("items", [])
    except Exception as e:
        print(f"[!] Error in google_search_all: {e}")
        return []

def contains_topic(url: str, topics: list[str]) -> bool:
    try:
        res = requests.get(url, timeout=5)
        text = res.text
        found = any(topic in text for topic in topics)
        #print(f"[+] Topic match: {found} for URL: {url}")
        return found
    except Exception as e:
        #print(f"[!] Error fetching {url}: {e}")
        return False
  
# store sent articles   
def load_sent_articles():
    if not os.path.exists(SENT_FILE):
        return []
    with open(SENT_FILE, "r") as f:
        return json.load(f)

def save_sent_articles(data):
    with open(SENT_FILE, "w") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def extract_clean_text(url: str) -> str:
    try:
        article = Article(url, language='ko')
        article.download()
        article.parse()
        return article.text.strip()
    except Exception as e:
        print(f"[!] newspaper3k failed for {url}: {e}")
        try:
            res = requests.get(url, timeout=5)
            soup = BeautifulSoup(res.text, "html.parser")
            text = soup.get_text(separator=' ', strip=True)
            return text[:3000]  # ë„ˆë¬´ ê¸¸ë©´ ìžë¦„
        except Exception as e2:
            print(f"[!] BeautifulSoup fallback failed for {url}: {e2}")
            return ""
        
# check for duplicate   
def is_similar_article(new_text: str, existing_summaries: list[str], threshold: float = 0.6) -> bool:
    def tokenize(text):
        return set(re.findall(r'\w+', text))

    new_tokens = tokenize(new_text)
    for old in existing_summaries:
        old_tokens = tokenize(old)
        if not old_tokens:
            continue
        similarity = len(new_tokens & old_tokens) / len(new_tokens | old_tokens)
        if similarity >= threshold:
            return True
    return False

async def push_news():
    bot = Bot(token=BOT_TOKEN)
    subscribers = load_subscribers()
    sent_articles = load_sent_articles()
    sent_summaries = [a["summary"] for a in sent_articles]

    for chat_id in subscribers:
        for company in companies:
            try:
                results = google_search_all(company, GOOGLE_API_KEY, GOOGLE_CSE_ID)
                print(f"[+] {company}: {len(results)} results")
                for item in results:
                    title = item.get("title", "")
                    snippet = item.get("snippet", "")
                    link = item.get("link", "")

                    if any(link == a["url"] for a in sent_articles):
                        print(f"â›”ï¸ Already sent: {link}")
                        continue

                    content = extract_clean_text(link)
                    if not content:
                        continue

                    summary = content[:1000]
                    if is_similar_article(summary, sent_summaries):
                        print(f"ðŸ” Similar to previous, skipping: {link}")
                        continue

                    topic_hits = [t for t in topics if t in content]
                    company_in_content = company in content

                    if (company_in_content and topic_hits) or len(topic_hits) >= 2:
                        msg = f"ðŸ” {company} + {len(topic_hits)}ê°œ í† í”½\n{title}\n{link}"
                        await bot.send_message(chat_id=chat_id, text=msg)
                        print(f"âœ… Sent to {chat_id}: {link}")

                        sent_articles.append({
                            "url": link,
                            "summary": summary,
                        })
                        sent_summaries.append(summary)
                        save_sent_articles(sent_articles)
                    else:
                        print(f"ðŸ”‡ ì¡°ê±´ ë¯¸ì¶©ì¡±: {link}")

            except Exception as e:
                print(f"[!] Error processing {company} for {chat_id}: {e}")

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "run-bot":
        app = ApplicationBuilder().token(BOT_TOKEN).build()
        app.add_handler(MessageHandler(filters.TEXT & (~filters.COMMAND), auto_subscribe))
        print("ðŸ¤– Bot is polling...")
        app.run_polling()
    else:
        asyncio.run(push_news())


'''
